{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ba03d10-3cb6-4c12-bf35-38d91f586329",
   "metadata": {},
   "source": [
    "# ME 592 Homework 4\n",
    "## Jake Bergfeld, Mohammad Rashid Mohammad Shoaib, Melika Tajipour\n",
    "#### Engineering Image Analysis - Distracted Driving Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffaf060-b848-4c4d-bf3a-faa8d00c4dcb",
   "metadata": {},
   "source": [
    "##### Gathering data from Kaggle - Link to data: https://www.kaggle.com/competitions/state-farm-distracted-driver-detection/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2043c07d-d5f3-4a9a-bdad-997953b54052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle\n",
    "# !mkdir .kaggle  #naming required by kaggle API, creates a hidden folder\n",
    "# !cp /home/exouser/Downloads/kaggle.json /home/exouser/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e61305-f1c6-4ec7-aeed-05bf56fd410e",
   "metadata": {},
   "source": [
    "##### <u>Confirming location of Kaggle API token was moved successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f80cc0b-31dd-41b4-9ad4-4d3287b7be60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle.json\n"
     ]
    }
   ],
   "source": [
    "!cd .kaggle && ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e602dcc-219e-4f97-b8a7-3a5e0bbc4aa0",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### <u>Downloading the specific dataset and confirming locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dac13c0-9863-4872-9dfb-1d1f0fde92b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets list -s 'State Farm Distracted Driver Detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17913b87-7eac-4816-bbbd-37330de551c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets download -d 'rightway11/state-farm-distracted-driver-detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6eee767-df29-4555-89b7-d3ddf35e66fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install unzip\n",
    "# !unzip state-farm-distracted-driver-detection.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b58dbde8-ca37-4bab-ad3b-520d4e30d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd data && ls\n",
    "# !cd data/imgs && ls\n",
    "# !cd data/imgs/train && ls\n",
    "# !cd data/imgs/test && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "defe1b6b-3a19-4fbb-a3c2-5c10d0a9e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import glob\n",
    "from torchvision.transforms import transforms\n",
    "# from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0f1f229-f4e4-4610-bede-7a416efa8b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_10206.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_27079.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_50749.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_97089.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_37741.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34919</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_15827.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34920</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_16688.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34921</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_64532.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34922</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_7918.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34923</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_84918.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34924 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject classname            img\n",
       "0        p012        c0  img_10206.jpg\n",
       "1        p012        c0  img_27079.jpg\n",
       "2        p012        c0  img_50749.jpg\n",
       "3        p012        c0  img_97089.jpg\n",
       "4        p012        c0  img_37741.jpg\n",
       "...       ...       ...            ...\n",
       "34919    p075        c9  img_15827.jpg\n",
       "34920    p075        c9  img_16688.jpg\n",
       "34921    p075        c9  img_64532.jpg\n",
       "34922    p075        c9   img_7918.jpg\n",
       "34923    p075        c9  img_84918.jpg\n",
       "\n",
       "[34924 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load driver details and image filenames\n",
    "driver_imgs = pd.read_csv('/home/exouser/data/driver_imgs_list.csv')\n",
    "driver_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21457688-bc7a-4318-8a67-8f6fe1b6539b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Data Information: \n",
    "    Default image size is 320x240\n",
    "    \n",
    "    The 10 classes to predict are:\n",
    "        c0: normal driving\n",
    "        c1: texting - right\n",
    "        c2: talking on the phone - right\n",
    "        c3: texting - left\n",
    "        c4: talking on the phone - left\n",
    "        c5: operating the radio\n",
    "        c6: drinking\n",
    "        c7: reaching behind\n",
    "        c8: hair and makeup\n",
    "        c9: talking to passenger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445889f4-515a-428a-837b-286c63032004",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Step 1: Train a model with roughly 500,000 parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bac34d6-d4ed-4505-9b8a-e9cb385062cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### *Formula to calculate the number of parameters in a CNN:*\n",
    "   -  Convolutional layer: (in_channels x out_channels x kernel_height x kernel_width) + out_channels\n",
    "   -  Batch normalization layer: 2 x num_features\n",
    "   -  ReLU activation layer: 0 (no parameters)\n",
    "   -  Max pooling layer: 0 (no parameters)\n",
    "   -  Fully connected layer: (in_features x out_features) + out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e64165c3-60aa-4ca9-80d2-e4d351e5fc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Check for nvidia device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd5ead62-f8cc-418c-a22c-034b5183e3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data transformation\n",
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.RandomHorizontalFlip(),   #Do this to add variation to data, augmentation technique\n",
    "    transforms.ToTensor(),              #Changes pixel range from color channel (0-255 to 0-1) changes from numpy to tensor\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], \n",
    "                         [0.5, 0.5, 0.5])     # 0-1 to [-1-1] \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61815e45-5c71-454d-8ed9-587eb37818a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataloader using batches\n",
    "\n",
    "#Directory path for training & testing data\n",
    "train_path = 'Documents/HW4/state-farm-distracted-driver-detection/imgs/train'\n",
    "test_path = 'Documents/HW4/state-farm-distracted-driver-detection/imgs/test'\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path, transform=transformer),\n",
    "    batch_size = 256, shuffle = True    #ADJUST THIS, HIGHER BATCH SIZE REQUIRES MORE MEMORY\n",
    ")\n",
    "\n",
    "# test_loader = DataLoader(\n",
    "#     torchvision.datasets.ImageFolder(test_path, transform=transformer),\n",
    "#     batch_size = 256, shuffle = True    #ADJUST THIS, HIGHER BATCH SIZE REQUIRES MORE MEMORY\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd7f28af-08d5-4a45-85e1-b4401b8e9384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n"
     ]
    }
   ],
   "source": [
    "#Categorize the images\n",
    "root = pathlib.Path(train_path)\n",
    "classes = sorted ([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c7b486d-7f6e-400e-ab7c-f8d0c731fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the CNN Network:\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "          \n",
    "        #Formula for height and width of CNN output: ((w-f+2P/s)+1)\n",
    "            # Where w = width (150), f = kernel size (3), P = padding (1), s = stride (1)\n",
    "        \n",
    "        #Input Shape = (256,3,150,150) in the format (batch size, RGB channel, image height, image width)\n",
    "        \n",
    "        #FIRST CNN LAYER:\n",
    "        self.conv1=nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        #New shape = (256,12,150,150)\n",
    "        self.bn1=nn.BatchNorm2d(num_features=12)\n",
    "        #New shape = (256,12,150,150)\n",
    "        self.reul1=nn.ReLU()\n",
    "        #New shape = (256,12,150,150)\n",
    "        #Now add max pooling layer\n",
    "        self.pool=nn.MaxPool2d(kernel_size=2)\n",
    "        #New shape = (256,12,75,75)\n",
    "        \n",
    "        #SECOND CNN LAYER:\n",
    "        self.conv2=nn.Conv2d(in_channels=12, out_channels=20, kernel_size=3, stride=1, padding=1)\n",
    "        #New shape = (256,20,75,75)\n",
    "        self.relu1=nn.ReLU()\n",
    "        #New shape =256,20,75,75)\n",
    "        \n",
    "        #THIRD CNN LAYER:\n",
    "        self.conv3=nn.Conv2d(in_channels=20, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        #New shape = (256,32,75,75)\n",
    "        self.bn1=nn.BatchNorm2d(num_features=32)\n",
    "        #New shape = (256,32,75,75)\n",
    "        self.reul1=nn.ReLU()\n",
    "        #New shape = (256,32,75,75)\n",
    "        \n",
    "        #PLAY AROUND WITH THESE LAYERS, CAN ADD MORE LAYERS OR MORE DEPTH TO INCREASE ACCURACY\n",
    "        self.fc=nn.Linear(in_features=12*75*75, out_features=num_classes)\n",
    "                          \n",
    "        \n",
    "        #Feed forward function\n",
    "    def forward(self,input):\n",
    "        output=self.conv1(input)\n",
    "        output=self.bn1(output)\n",
    "        output=self.relu1(output)\n",
    "        output=self.pool(output)\n",
    "            \n",
    "        output=self.conv2(output)\n",
    "        output=self.relu2(output)\n",
    "            \n",
    "        output=self.conv3(output)\n",
    "        output=self.bn3(output)\n",
    "        output=self.relu3(output)\n",
    "        #This generates an matrix output with shape: (256,32,75,75)\n",
    "            \n",
    "        output=output.view(-1,32*75*75)\n",
    "            \n",
    "        output=self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9f9f4e7-c925-4094-8241-ffece08750ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ConvNet(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16c14bfd-ff62-495b-a504-761a1bb02202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer & Loss function:\n",
    "optimizer=Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "loss_function=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cb0bbb4-5ce9-417a-9c7d-e9e991369e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter epoch count\n",
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b45a898-c521-46c4-8a07-034411cad7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the size of training & testing images\n",
    "train_count=len(glob.glob(train_path+'/**/*.jpg'))\n",
    "# test_count=len(glob.glob(test_path+'/**/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab1569fa-9191-463a-9dc5-a046fea2ce6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22424\n"
     ]
    }
   ],
   "source": [
    "print(train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb29a07c-5112-4f20-9b91-2bd5ffd6961e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "...\n",
    "\n",
    "# Reset batch normalization layer parameters\n",
    "model.apply(lambda m: m.reset_parameters() if isinstance(m, nn.BatchNorm2d) else None)\n",
    "\n",
    "#Training CNN network and saving best model\n",
    "best_accuracy=0.0\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23f16f4e-9588-4919-9a64-a0d718f27213",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 12 elements not 32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4107/2251421988.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4107/834790795.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \"\"\"\n\u001b[0;32m--> 171\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2448\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2450\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2451\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 12 elements not 32"
     ]
    }
   ],
   "source": [
    "#Training CNN network and saving best model\n",
    "best_accuracy=0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #Evaluation & training for training data\n",
    "    model.train()\n",
    "    training_accuracy=0.0\n",
    "    train_loss=0.0\n",
    "    \n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        outputs=model(images)\n",
    "        loss=loss_function(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss+=loss.cpu().data*images.size(0)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    train_accuracy=train_accuracy/train_count\n",
    "    train_loss=train_loss/train_count\n",
    "    \n",
    "    #Evaluation & training for test data\n",
    "    model.eval()\n",
    "    \n",
    "    for i, (images,labels) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "        \n",
    "        outputs=model(images)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    test_accuracy=test_accuracy/test_count\n",
    "    \n",
    "    print('Epoch: '+str(epoch)+' Train Loss: '+str(int(train_loss))+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\n",
    "    \n",
    "    # #Save the best model\n",
    "    # if test_accuracy>best_accuracy:\n",
    "    #     torch.save(model.state_dict(),'best_checkpoint.model')\n",
    "    #     best_accuracy=test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c404fa7-e8de-4545-ac93-0beb6d37cb1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3551d433-b6a3-4f59-bf62-8e8003aca683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af9827-9470-4e21-8fca-34af0ce230db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3534fc4-c2ee-4216-9c90-e523abf228de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8409dac9-ebec-4a0d-be93-5c5b0e5966dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecaed3e-4117-41ab-824f-14a03ba2eb73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34db195-8eea-42d1-a59e-a68e053b134b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecf5fe55-198e-467b-a279-f9b616406bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@2064.225] global loadsave.cpp:244 findDecoder imread_('statefarm-distracted-driver-detection/train/c2/img_14717.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'img_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4107/898701554.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_driver_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mvalid_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_driver_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4107/898701554.py\u001b[0m in \u001b[0;36mpreprocess_images\u001b[0;34m(driver_imgs)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'statefarm-distracted-driver-detection/train/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdriver_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'classname'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdriver_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'img'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdriver_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'classname'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4107/898701554.py\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img_size' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert the class labels to integers\n",
    "class_names = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n",
    "class_dict = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "# Shuffle the data\n",
    "driver_imgs = driver_imgs.sample(frac=1, random_state=42)\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "train_size = int(0.8 * len(driver_imgs))\n",
    "train_driver_imgs = driver_imgs[:train_size].reset_index(drop=True)\n",
    "valid_driver_imgs = driver_imgs[train_size:].reset_index(drop=True)\n",
    "\n",
    "# Load and pre-process the images\n",
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, img_size)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "def preprocess_images(driver_imgs):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(driver_imgs)):\n",
    "        image_path = 'statefarm-distracted-driver-detection/train/' + driver_imgs.loc[i, 'classname'] + '/' + driver_imgs.loc[i, 'img']\n",
    "        image = load_image(image_path)\n",
    "        images.append(image)\n",
    "        labels.append(class_dict[driver_imgs.loc[i, 'classname']])\n",
    "\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "train_images, train_labels = preprocess_images(train_driver_imgs)\n",
    "valid_images, valid_labels = preprocess_images(valid_driver_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e0bb7fc7-281e-4243-8dfe-5fb180ee7bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check for nvidia device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "27fef16c-7aec-403f-8914-4d80ae74a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation\n",
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "066a099c-0ff3-4a56-aeea-3866aef5ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8a5013-eced-4481-80d2-198d4b3b7752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader using batches\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path, transform=transformer),\n",
    "    batch_size=256, shuffle=True\n",
    ")\n",
    "\n",
    "# test_loader = DataLoader(\n",
    "#     torchvision.datasets.ImageFolder(test_path, transform=transformer),\n",
    "#     batch_size=256, shuffle=False\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf9e225-b29f-4f78-bdc3-31e19347b31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize the images\n",
    "root = pathlib.Path(train_path)\n",
    "classes = [j.name.split('/')[-1] for j in root.iterdir()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b38930-10ea-4d44-bcf3-866fca336c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the CNN Network:\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "          \n",
    "        #Formula for height and width of CNN output: ((w-f+2P/s)+1)\n",
    "            # Where w = width (150), f = kernel size (3), P = padding (1), s = stride (1)\n",
    "        \n",
    "        #Input Shape = (256,3,150,150) in the format (batch size, RGB channel, image height, image width)\n",
    "        \n",
    "        #FIRST CNN LAYER:\n",
    "        self.conv1=nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        #New shape = (256,12,150,150)\n",
    "        self.bn1=nn.BatchNorm2d(num_features=12)\n",
    "        #New shape = (256,12,150,150)\n",
    "        self.reul1=nn.ReLU()\n",
    "        #New shape = (256,12,150,150)\n",
    "        #Now add max pooling layer\n",
    "        self.pool=nn.MaxPool2d(kernel_size=2)\n",
    "        #New shape = (256,12,75,75)\n",
    "        \n",
    "        #SECOND CNN LAYER:\n",
    "        self.conv2=nn.Conv2d(in_channels=12, out_channels=20, kernel_size=3, stride=1, padding=1)\n",
    "        #New shape = (256,20,75,75)\n",
    "        self.relu1=nn.ReLU()\n",
    "        #New shape =256,20,75,75)\n",
    "        \n",
    "        #THIRD CNN LAYER:\n",
    "        self.conv3=nn.Conv2d(in_channels=20, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        #New shape = (256,32,75,75)\n",
    "        self.bn1=nn.BatchNorm2d(num_features=32)\n",
    "        #New shape = (256,32,75,75)\n",
    "        self.reul1=nn.ReLU()\n",
    "        #New shape = (256,32,75,75)\n",
    "        \n",
    "        #PLAY AROUND WITH THESE LAYERS, CAN ADD MORE LAYERS OR MORE DEPTH TO INCREASE ACCURACY\n",
    "        self.fc=nn.Linear(in_features=32*75*75, out_features=num_classes)\n",
    "                          \n",
    "        \n",
    "        #Feed forward function\n",
    "    def forward(self,input):\n",
    "        output=self.conv1(input)\n",
    "        output=self.bn1(output)\n",
    "        output=self.relu1(output)\n",
    "        output=self.pool(output)\n",
    "            \n",
    "        output=self.conv2(output)\n",
    "        output=self.relu2(output)\n",
    "            \n",
    "        output=self.conv3(output)\n",
    "        output=self.bn3(output)\n",
    "        output=self.relu3(output)\n",
    "        #This generates an matrix output with shape: (256,32,75,75)\n",
    "            \n",
    "        output=output.view(-1,32*75*75)\n",
    "            \n",
    "        output=self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffed80c-8c32-45e2-b92b-072455aac295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Saving the trained model\n",
    "# torch.save(model.state_dict(),'model.ckpt')\n",
    "# print('Model trained and saved successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e991690-0525-4632-97f3-18470baad9dd",
   "metadata": {},
   "source": [
    "### <u>Step 2: Train a model with roughly 10,00,000 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c402311-65e3-4fbd-bd71-6da8325a2860",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
