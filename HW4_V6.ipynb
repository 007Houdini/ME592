{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f668866b-3d18-4cdb-982c-7620c2310277",
   "metadata": {},
   "source": [
    "# ME 592 Homework 4\n",
    "## Jake Bergfeld, Mohammad Rashid Mohammad Shoaib, Melika Tajipour\n",
    "#### Engineering Image Analysis - Distracted Driving Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23bf84f-aa9b-4c03-a813-0cc567b7eb1d",
   "metadata": {},
   "source": [
    "##### Gathering data from Kaggle - Link to data: https://www.kaggle.com/competitions/state-farm-distracted-driver-detection/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f366cb74-6968-4bdc-a682-809936a7fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle\n",
    "# !mkdir .kaggle  #naming required by kaggle API, creates a hidden folder\n",
    "# !cp /home/exouser/Downloads/kaggle.json /home/exouser/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af339634-b8fb-4500-afb5-9e10930a4faf",
   "metadata": {},
   "source": [
    "##### <u>Confirming location of Kaggle API token was moved successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0b17d81-48f1-4ffc-9742-ec0efc1bb5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd .kaggle && ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93d62f-8c6e-4a4e-b854-fd3f45e0d627",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### <u>Downloading the specific dataset and confirming locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52f27c40-28c7-4ea4-9997-f23b63576322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets list -s 'State Farm Distracted Driver Detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "908d63c8-fd0f-42dd-8d85-5d3f2498ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets download -d 'rightway11/state-farm-distracted-driver-detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4e555fa-03f7-4ac1-96db-d09b39dceb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install unzip\n",
    "# !unzip state-farm-distracted-driver-detection.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfd5be0c-a808-42d2-8527-cb92926eb100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd data && ls\n",
    "# !cd data/imgs && ls\n",
    "# !cd data/imgs/train && ls\n",
    "# !cd data/imgs/test && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6e19d85-a8d7-42e7-9658-e5f1dd2e1ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import glob\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "261a30d3-4797-4656-98f0-dce695c10c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_10206.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_27079.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_50749.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_97089.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_37741.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34919</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_15827.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34920</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_16688.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34921</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_64532.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34922</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_7918.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34923</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_84918.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34924 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject classname            img\n",
       "0        p012        c0  img_10206.jpg\n",
       "1        p012        c0  img_27079.jpg\n",
       "2        p012        c0  img_50749.jpg\n",
       "3        p012        c0  img_97089.jpg\n",
       "4        p012        c0  img_37741.jpg\n",
       "...       ...       ...            ...\n",
       "34919    p075        c9  img_15827.jpg\n",
       "34920    p075        c9  img_16688.jpg\n",
       "34921    p075        c9  img_64532.jpg\n",
       "34922    p075        c9   img_7918.jpg\n",
       "34923    p075        c9  img_84918.jpg\n",
       "\n",
       "[34924 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load driver details and image filenames\n",
    "driver_imgs = pd.read_csv('/home/exouser/data/driver_imgs_list.csv')\n",
    "driver_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabefe56-95ad-4af1-9ca8-c2d73047ff36",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Data Information: \n",
    "    Default image size is 320x240\n",
    "    \n",
    "    The 10 classes to predict are:\n",
    "        c0: normal driving\n",
    "        c1: texting - right\n",
    "        c2: talking on the phone - right\n",
    "        c3: texting - left\n",
    "        c4: talking on the phone - left\n",
    "        c5: operating the radio\n",
    "        c6: drinking\n",
    "        c7: reaching behind\n",
    "        c8: hair and makeup\n",
    "        c9: talking to passenger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cf7055-8831-48a0-8e97-d438ddd51f2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Step 1: Train a model with roughly 500,000 parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f816f658-c0b1-4857-b150-09636a3176f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### *Formula to calculate the number of parameters in a CNN:*\n",
    "   -  Convolutional layer: (in_channels x out_channels x kernel_height x kernel_width) + out_channels\n",
    "   -  Batch normalization layer: 2 x num_features\n",
    "   -  ReLU activation layer: 0 (no parameters)\n",
    "   -  Max pooling layer: 0 (no parameters)\n",
    "   -  Fully connected layer: (in_features x out_features) + out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8848819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98d72611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#checking for device\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3068b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms\n",
    "transformer=transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
    "                        [0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23f61d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataloader\n",
    "\n",
    "#Path for training and testing directory\n",
    "train_path='/home/exouser/data/imgs/train'\n",
    "# test_path='/home/exouser/data/imgs/test'\n",
    "\n",
    "\n",
    "train_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path,transform=transformer),\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "# test_loader=DataLoader(\n",
    "#     torchvision.datasets.ImageFolder(test_path,transform=transformer),\n",
    "#     batch_size=32, shuffle=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d64520e9-0aa3-441f-b4ae-122e8115df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Breaking the labeled training data into 2 datasets: Training & labeled Testing images\n",
    "# path = '/home/exouser/data/imgs/train (copy)'\n",
    "\n",
    "# folder1 = '/home/exouser/data/imgs/train_copy/train_imgs'    #70% ofthe training images will be moved here\n",
    "# folder2 = '/home/exouser/data/imgs/train_copy/test_imgs'     #30% of the training images will be moved here\n",
    "\n",
    "# image_files = os.listdir(path)\n",
    "# random.shuffle(image_files)\n",
    "\n",
    "# split_index = int(0.7*len(image_files))   #70/30 split\n",
    "# image_set1 = image_files[:split_index]\n",
    "# image_set2 = image_files[split_index:]\n",
    "\n",
    "# for image_file in image_set1:\n",
    "#     shutil.move(os.path.join(path,image_file), folder1)\n",
    "    \n",
    "# for image_file in image_set2:\n",
    "#     shutil.move(os.path.join(path, image_file), folder2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1451f45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n"
     ]
    }
   ],
   "source": [
    "#categories\n",
    "root=pathlib.Path(train_path)\n",
    "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "print(classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "555510dc-c115-4faf-bc88-b992219c399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a CNN Network with ~500,000 parameters\n",
    "\n",
    "class MoreParamsConvNet(nn.Module):\n",
    "    def __init__(self,num_classes=10):\n",
    "        super(MoreParamsConvNet,self).__init__()\n",
    "\n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        self.bn1=nn.BatchNorm2d(num_features=32)\n",
    "        self.relu1=nn.ReLU()\n",
    "\n",
    "        self.conv2=nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=1,padding=1)\n",
    "        self.bn2=nn.BatchNorm2d(num_features=64)\n",
    "        self.relu2=nn.ReLU()\n",
    "\n",
    "        self.conv3=nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=1,padding=1)\n",
    "        self.bn3=nn.BatchNorm2d(num_features=128)\n",
    "        self.relu3=nn.ReLU()\n",
    "\n",
    "        self.conv4=nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,stride=1,padding=1)\n",
    "        self.bn4=nn.BatchNorm2d(num_features=256)\n",
    "        self.relu4=nn.ReLU()\n",
    "\n",
    "        self.pool=nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.fc1=nn.Linear(in_features=256*18*18,out_features=512)\n",
    "        self.relu5=nn.ReLU()\n",
    "\n",
    "        self.fc2=nn.Linear(in_features=512,out_features=num_classes)\n",
    "\n",
    "        # Total parameter count for the CNN: 32*3*3*3 + 32 + 64*32*3*3 + 64 + 128*64*3*3 + 128 + 256*128*3*3 + 256 + 256*18*18*512 + 512 + 512*10 = 492,532\n",
    "    \n",
    "    def forward(self,input):\n",
    "        output=self.conv1(input)\n",
    "        output=self.bn1(output)\n",
    "        output=self.relu1(output)\n",
    "\n",
    "        output=self.pool(output)\n",
    "\n",
    "        output=self.conv2(output)\n",
    "        output=self.bn2(output)\n",
    "        output=self.relu2(output)\n",
    "\n",
    "        output=self.pool(output)\n",
    "\n",
    "        output=self.conv3(output)\n",
    "        output=self.bn3(output)\n",
    "        output=self.relu3(output)\n",
    "    \n",
    "        output=self.pool(output)\n",
    "\n",
    "        output=self.conv4(output)\n",
    "        output=self.bn4(output)\n",
    "        output=self.relu4(output)\n",
    "\n",
    "        output=output.view(-1,256*18*18)\n",
    "\n",
    "        output=self.fc1(output)\n",
    "        output=self.relu5(output)\n",
    "\n",
    "        output=self.fc2(output)\n",
    "\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a5808ed3-1d8b-4010-9133-4fcf54ceabbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MoreParamsConvNet(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3be2b8c0-4d74-40e8-8ac9-e5f791d19f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optmizer and loss function\n",
    "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
    "loss_function=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "39445d39-caab-48bd-b06c-380d217bd508",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ca694c5f-8553-46a6-ba90-66432d877686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the size of training and testing images\n",
    "train_count=len(glob.glob(train_path+'/**/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f44c7ae9-1ea5-4e81-bde5-7345f722dae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17462\n"
     ]
    }
   ],
   "source": [
    "print(train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ddb983a3-91b4-4031-aaef-eed3ae6c9e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: tensor(2.6189) Train Accuracy: 0.46512426984308786\n",
      "Epoch: 1 Train Loss: tensor(0.3767) Train Accuracy: 0.8866681937922346\n",
      "Epoch: 2 Train Loss: tensor(0.1327) Train Accuracy: 0.9620318405680907\n",
      "Epoch: 3 Train Loss: tensor(0.0775) Train Accuracy: 0.9772649181078914\n",
      "Epoch: 4 Train Loss: tensor(0.0551) Train Accuracy: 0.9839651815370519\n",
      "Epoch: 5 Train Loss: tensor(0.0501) Train Accuracy: 0.9841942503722368\n",
      "Epoch: 6 Train Loss: tensor(0.0535) Train Accuracy: 0.9831634406139045\n",
      "Epoch: 7 Train Loss: tensor(0.0479) Train Accuracy: 0.9859122666361242\n",
      "Epoch: 8 Train Loss: tensor(0.0380) Train Accuracy: 0.9884892910319552\n",
      "Epoch: 9 Train Loss: tensor(0.0539) Train Accuracy: 0.9833352422402932\n"
     ]
    }
   ],
   "source": [
    "#Model training and saving best model\n",
    "\n",
    "best_accuracy=0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #Evaluation and training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy=0.0\n",
    "    train_loss=0.0\n",
    "    \n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs=model(images)\n",
    "        loss=loss_function(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        train_loss+= loss.cpu().data*images.size(0)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        \n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    train_accuracy=train_accuracy/train_count\n",
    "    train_loss=train_loss/train_count\n",
    "    \n",
    "    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy))\n",
    "\n",
    "    \n",
    "#     # Evaluation on testing dataset\n",
    "#     model.eval()\n",
    "    \n",
    "#     test_accuracy=0.0\n",
    "#     for i, (images,labels) in enumerate(test_loader):\n",
    "#         if torch.cuda.is_available():\n",
    "#             images=Variable(images.cuda())\n",
    "#             labels=Variable(labels.cuda())\n",
    "            \n",
    "#         outputs=model(images)\n",
    "#         _,prediction=torch.max(outputs.data,1)\n",
    "#         test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "    \n",
    "#     test_accuracy=test_accuracy/test_count\n",
    "    \n",
    "    \n",
    "#     print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\n",
    "#     print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy))\n",
    "\n",
    "#     #Save the best model\n",
    "#     if test_accuracy>best_accuracy:\n",
    "#         torch.save(model.state_dict(),'best_checkpoint.model')\n",
    "#         best_accuracy=test_accuracy\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab2b06a-64dd-4ecc-b671-b66cab05236c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c928830d-34e4-4fb8-8317-6174f9556797",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Step 2: Train a model with roughly 10,000,000 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "797a001f-9752-4a0e-abab-b441f1c21db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LargeConvNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=256)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=1024)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=1024 * 8 * 8, out_features=2048)\n",
    "        self.relu6 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=2048, out_features=num_classes)\n",
    "\n",
    "        # Total parameter count for the CNN: 64*3*3*3*3 + 64 + 128*64*5*5*3 + 128 + 256*128*3*3*1 + 256 + 512*256*3*3*1 + 512 + 1024*512*3*3*1 + 1024 + 1024*8*8*2048 + 2048 + 2048*10 \n",
    "        # = 10,041,866\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv1(input)\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = self.conv2(output)\n",
    "        output = self.bn2(output)\n",
    "        output = self.relu2(output)\n",
    "\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = self.conv3(output)\n",
    "        output = self.bn3(output)\n",
    "        output = self.relu3(output)\n",
    "\n",
    "        output = self.conv4(output)\n",
    "        output = self.bn4(output)\n",
    "        output = self.relu4(output)\n",
    "\n",
    "        output = self.conv5(output)\n",
    "        output = self.bn5(output)\n",
    "        output = self.relu5(output)\n",
    "\n",
    "        output = output.view(-1, 1024 * 8 * 8)\n",
    "\n",
    "        output = self.fc1(output)\n",
    "        output = self.relu6(output)\n",
    "\n",
    "        output = self.fc2(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cf3e954b-dfc0-4e0a-9c1e-7f1736ca9e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LargeConvNet(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e1f7296c-42d1-4592-b2d9-1b503ca3908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optmizer and loss function\n",
    "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
    "loss_function=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "225b997a-76a3-4260-b778-5602191d3282",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4590182d-8148-4314-9926-a4f6adeb7a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the size of training and testing images\n",
    "train_count=len(glob.glob(train_path+'/**/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "333f3e99-1053-46a8-aa9e-53d1925c0c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17462\n"
     ]
    }
   ],
   "source": [
    "print(train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c0d04a05-d192-457b-85a1-129c06e932ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (81) to match target batch_size (64).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7114/3837181423.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (81) to match target batch_size (64)."
     ]
    }
   ],
   "source": [
    "#Model training and saving best model\n",
    "\n",
    "best_accuracy=0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #Evaluation and training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy=0.0\n",
    "    train_loss=0.0\n",
    "    \n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs=model(images)\n",
    "        loss=loss_function(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        train_loss+= loss.cpu().data*images.size(0)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        \n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    train_accuracy=train_accuracy/train_count\n",
    "    train_loss=train_loss/train_count\n",
    "    \n",
    "    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy))\n",
    "\n",
    "    \n",
    "#     # Evaluation on testing dataset\n",
    "#     model.eval()\n",
    "    \n",
    "#     test_accuracy=0.0\n",
    "#     for i, (images,labels) in enumerate(test_loader):\n",
    "#         if torch.cuda.is_available():\n",
    "#             images=Variable(images.cuda())\n",
    "#             labels=Variable(labels.cuda())\n",
    "            \n",
    "#         outputs=model(images)\n",
    "#         _,prediction=torch.max(outputs.data,1)\n",
    "#         test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "    \n",
    "#     test_accuracy=test_accuracy/test_count\n",
    "    \n",
    "    \n",
    "#     print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\n",
    "#     print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy))\n",
    "\n",
    "#     #Save the best model\n",
    "#     if test_accuracy>best_accuracy:\n",
    "#         torch.save(model.state_dict(),'best_checkpoint.model')\n",
    "#         best_accuracy=test_accuracy\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62af0c5c-1cae-4478-a183-90fc541aac84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
