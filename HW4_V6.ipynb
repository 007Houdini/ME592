{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f668866b-3d18-4cdb-982c-7620c2310277",
   "metadata": {},
   "source": [
    "# ME 592 Homework 4\n",
    "## Jake Bergfeld, Mohammad Rashid Mohammad Shoaib, Melika Tajipour\n",
    "#### Engineering Image Analysis - Distracted Driving Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23bf84f-aa9b-4c03-a813-0cc567b7eb1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Gathering data from Kaggle - Link to data: https://www.kaggle.com/competitions/state-farm-distracted-driver-detection/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f366cb74-6968-4bdc-a682-809936a7fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle\n",
    "# !mkdir .kaggle  #naming required by kaggle API, creates a hidden folder\n",
    "# !cp /home/exouser/Downloads/kaggle.json /home/exouser/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af339634-b8fb-4500-afb5-9e10930a4faf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### <u>Confirming location of Kaggle API token was moved successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0b17d81-48f1-4ffc-9742-ec0efc1bb5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd .kaggle && ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93d62f-8c6e-4a4e-b854-fd3f45e0d627",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### <u>Downloading the specific dataset and confirming locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52f27c40-28c7-4ea4-9997-f23b63576322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets list -s 'State Farm Distracted Driver Detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "908d63c8-fd0f-42dd-8d85-5d3f2498ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets download -d 'rightway11/state-farm-distracted-driver-detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4e555fa-03f7-4ac1-96db-d09b39dceb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install unzip\n",
    "# !unzip state-farm-distracted-driver-detection.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfd5be0c-a808-42d2-8527-cb92926eb100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd data && ls\n",
    "# !cd data/imgs && ls\n",
    "# !cd data/imgs/train && ls\n",
    "# !cd data/imgs/test && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "261a30d3-4797-4656-98f0-dce695c10c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_10206.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_27079.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_50749.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_97089.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_37741.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34919</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_15827.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34920</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_16688.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34921</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_64532.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34922</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_7918.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34923</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_84918.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34924 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject classname            img\n",
       "0        p012        c0  img_10206.jpg\n",
       "1        p012        c0  img_27079.jpg\n",
       "2        p012        c0  img_50749.jpg\n",
       "3        p012        c0  img_97089.jpg\n",
       "4        p012        c0  img_37741.jpg\n",
       "...       ...       ...            ...\n",
       "34919    p075        c9  img_15827.jpg\n",
       "34920    p075        c9  img_16688.jpg\n",
       "34921    p075        c9  img_64532.jpg\n",
       "34922    p075        c9   img_7918.jpg\n",
       "34923    p075        c9  img_84918.jpg\n",
       "\n",
       "[34924 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load driver details and image filenames\n",
    "driver_imgs = pd.read_csv('/home/exouser/data/driver_imgs_list.csv')\n",
    "driver_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8754332c-ab4c-4fe1-b78c-9badb0ef26d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### <u>Data Information: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1e33ac-8e2d-45b2-9e5a-f9e47b7bf167",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Default image size is 320x240\n",
    "    \n",
    "##### The 10 classes to predict are:\n",
    "   - c0: normal driving\n",
    "   - c1: texting - right\n",
    "   - c2: talking on the phone - right\n",
    "   - c3: texting - left\n",
    "   - c4: talking on the phone - left\n",
    "   - c5: operating the radio\n",
    "   - c6: drinking\n",
    "   - c7: reaching behind\n",
    "   - c8: hair and makeup\n",
    "   - c9: talking to passenger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f816f658-c0b1-4857-b150-09636a3176f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### *Formula to calculate the number of parameters in a CNN:*\n",
    "   -  Convolutional layer: (in_channels x out_channels x kernel_height x kernel_width) + out_channels\n",
    "   -  Batch normalization layer: 2 x num_features\n",
    "   -  ReLU activation layer: 0 (no parameters)\n",
    "   -  Max pooling layer: 0 (no parameters)\n",
    "   -  Fully connected layer: (in_features x out_features) + out_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3191612b-b2f7-49bc-9511-95e7af33e161",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Library Imports & Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e8848819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "98d72611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#checking for device\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "3068b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the input images\n",
    "transformer=transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
    "                        [0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6460334-571a-4c5d-a2b6-a424446aa3fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### <u>Generating Test Data\n",
    "#### Only needs to be run one time at the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c35a7cb6-a911-45bf-9be8-014438fe8e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Only run this once to generate test and train data and create separate folders for each\n",
    "\n",
    "# import os\n",
    "# import random\n",
    "# import shutil\n",
    "\n",
    "# # Set the path to the 'Train & Test' folder\n",
    "# folder_path = '/home/exouser/data/imgs/Train & Test'\n",
    "\n",
    "# # Set the path to the 'Train' folder\n",
    "# train_path = '/home/exouser/data/imgs/Train & Test/Train'\n",
    "\n",
    "# # Set the path to the 'Test' folder\n",
    "# test_path = '/home/exouser/data/imgs/Train & Test/Test'\n",
    "\n",
    "# # Loop through each subfolder in the 'Train & Test' folder\n",
    "# for subfolder in os.listdir(folder_path):\n",
    "\n",
    "#     # Create a new subfolder in the 'Train' folder with the same name\n",
    "#     train_subfolder = os.path.join(train_path, subfolder)\n",
    "#     os.makedirs(train_subfolder, exist_ok=True)\n",
    "\n",
    "#     # Create a new subfolder in the 'Test' folder with the same name\n",
    "#     test_subfolder = os.path.join(test_path, subfolder)\n",
    "#     os.makedirs(test_subfolder, exist_ok=True)\n",
    "\n",
    "#     # List all the image files in the subfolder\n",
    "#     images = os.listdir(os.path.join(folder_path, subfolder))\n",
    "#     random.shuffle(images)\n",
    "\n",
    "#     # Calculate the number of images to move to the 'Train' folder\n",
    "#     split_index = int(len(images) * 0.7)\n",
    "\n",
    "#     # Move the first 70% of images to the 'Train' folder\n",
    "#     for image in images[:split_index]:\n",
    "#         source = os.path.join(folder_path, subfolder, image)\n",
    "#         destination = os.path.join(train_subfolder, image)\n",
    "#         shutil.copyfile(source, destination)\n",
    "\n",
    "#     # Move the remaining 30% of images to the 'Test' folder\n",
    "#     for image in images[split_index:]:\n",
    "#         source = os.path.join(folder_path, subfolder, image)\n",
    "#         destination = os.path.join(test_subfolder, image)\n",
    "#         shutil.copyfile(source, destination)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6737a4a2-0e5e-4138-93cc-2708be9b71b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Dataloader and Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "23f61d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataloader\n",
    "\n",
    "#Directory path for the training & test images\n",
    "train_path='/home/exouser/data/imgs/Train & Test/Train'\n",
    "test_path='/home/exouser/data/imgs/Train & Test/Test'\n",
    "\n",
    "train_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path,transform=transformer),\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "test_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(test_path,transform=transformer),\n",
    "    batch_size=32, shuffle=True\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "1451f45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n"
     ]
    }
   ],
   "source": [
    "#categories\n",
    "root=pathlib.Path(train_path)\n",
    "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "print(classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a48aa72-d6a0-4fbc-a032-0466c2def2d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Step 1: Build and train a CNN model with roughly 500,000 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "0f66a126-b1d8-437b-b35c-ba0687eebf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a CNN Network with ~500,000 parameters\n",
    "\n",
    "class ConvNet500k(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet500k, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=128 * 18 * 18, out_features=1024)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv1(input)\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = self.conv2(output)\n",
    "        output = self.bn2(output)\n",
    "        output = self.relu2(output)\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = self.conv3(output)\n",
    "        output = self.bn3(output)\n",
    "        output = self.relu3(output)\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = output.view(-1, 128 * 18 * 18)\n",
    "        output = self.fc1(output)\n",
    "        output = self.relu4(output)\n",
    "        output = self.fc2(output)\n",
    "\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "46da7c29-acae-480a-99b1-9db90d797efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ConvNet500k(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "45426278-69e0-4594-8f61-2daebb00c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optmizer and loss function\n",
    "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
    "loss_function=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "72f44e2a-937d-4915-9110-0adb7a0e3d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "8bb34ac1-2e0e-4459-9d90-b8f6866b61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the size of training and testing images\n",
    "train_count=len(glob.glob(train_path+'/**/*.jpg'))\n",
    "test_count=len(glob.glob(test_path+'/**/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "0a70464a-7d9b-4734-b181-9c5cfb826d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12218\n",
      "5244\n"
     ]
    }
   ],
   "source": [
    "print(train_count)\n",
    "print(test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "12fa5655-120b-41c0-b1d7-4ccb6769efb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: tensor(3.9341) Train Accuracy: 0.29120969062039614 Test Accuracy: 0.526697177726926\n",
      "Epoch: 1 Train Loss: tensor(0.8982) Train Accuracy: 0.7053527582255689 Test Accuracy: 0.8039664378337147\n",
      "Epoch: 2 Train Loss: tensor(0.3272) Train Accuracy: 0.9048944180716975 Test Accuracy: 0.9445080091533181\n",
      "Epoch: 3 Train Loss: tensor(0.1598) Train Accuracy: 0.9558029137338353 Test Accuracy: 0.9546147978642258\n",
      "Epoch: 4 Train Loss: tensor(0.1043) Train Accuracy: 0.9686528073334425 Test Accuracy: 0.9649122807017544\n",
      "Epoch: 5 Train Loss: tensor(0.0745) Train Accuracy: 0.9785562285153053 Test Accuracy: 0.9639588100686499\n",
      "Epoch: 6 Train Loss: tensor(0.0567) Train Accuracy: 0.9842036339826485 Test Accuracy: 0.975209763539283\n",
      "Epoch: 7 Train Loss: tensor(0.0474) Train Accuracy: 0.985922409559666 Test Accuracy: 0.8674675819984744\n",
      "Epoch: 8 Train Loss: tensor(0.0507) Train Accuracy: 0.984449173350794 Test Accuracy: 0.975209763539283\n",
      "Epoch: 9 Train Loss: tensor(0.0413) Train Accuracy: 0.986986413488296 Test Accuracy: 0.9717772692601068\n"
     ]
    }
   ],
   "source": [
    "#Model training and saving best model\n",
    "\n",
    "best_accuracy=0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #Evaluation and training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy=0.0\n",
    "    train_loss=0.0\n",
    "    \n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs=model(images)\n",
    "        loss=loss_function(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        train_loss+= loss.cpu().data*images.size(0)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        \n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    train_accuracy=train_accuracy/train_count\n",
    "    train_loss=train_loss/train_count\n",
    "    \n",
    "    # print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy))\n",
    "\n",
    "    \n",
    "    # Evaluation on testing dataset\n",
    "    model.eval()\n",
    "    \n",
    "    test_accuracy=0.0\n",
    "    for i, (images,labels) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        outputs=model(images)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "    \n",
    "    test_accuracy=test_accuracy/test_count\n",
    "    \n",
    "    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\n",
    "\n",
    "    #Save the best model\n",
    "    if test_accuracy>best_accuracy:\n",
    "        torch.save(model.state_dict(),'best_checkpoint.model')\n",
    "        best_accuracy=test_accuracy\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050f36d8-66ea-4310-8187-443df0f6d664",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Now using unlabeled images to evaluate the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "bcbd763a-5005-4aef-80a6-d24cb625643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading more libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import os\n",
    "import pathlib\n",
    "import glob\n",
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.models import squeezenet1_1\n",
    "from io import open\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "ab6f6550-301e-443d-bdc9-f427f039e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path='/home/exouser/data/imgs/Train & Test/Predict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "fe4580c9-63da-4b64-b16b-9a70e185e33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet500k(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc1): Linear(in_features=41472, out_features=1024, bias=True)\n",
       "  (relu4): ReLU()\n",
       "  (fc2): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint=torch.load('best_checkpoint.model')\n",
    "model=ConvNet500k(num_classes=10)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "432b5041-9976-4111-9de3-7669fb01620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the prediction input images\n",
    "transformer=transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
    "                        [0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "ed22f6b3-446d-429e-a405-a6df5f2c2e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction function\n",
    "def prediction(img_path,transformer):\n",
    "    \n",
    "    image=Image.open(img_path)\n",
    "    image_tensor=transformer(image).float()\n",
    "    image_tensore=image_tensor.unsqueeze_(0)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        image_tensor.cuda()\n",
    "        \n",
    "    input=Variable(image_tensors)\n",
    "    \n",
    "    output=model(input)\n",
    "    index=output.data.numpy().argmax()\n",
    "    pred=classes[index]\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "a40e773a-8b23-48ee-90bd-ebf93457d375",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path=glob.glob(pred_path+'/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "c662abda-83ec-4636-a0e6-e5e33696f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict={}\n",
    "\n",
    "for i in images_path:\n",
    "    pred_dict[i[i.rfind('/')+1:]]=prediction(i,transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "2eb75946-23eb-4786-91c3-ad654f1d6255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fbe686-610f-452d-8e80-d72c13700200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c83bca-e636-4eb4-9a69-f052fb140e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c928830d-34e4-4fb8-8317-6174f9556797",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Step 2: Train a model with roughly 10,000,000 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "797a001f-9752-4a0e-abab-b441f1c21db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet10M(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet10M, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=256)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=256 * 18 * 18, out_features=4096)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=4096, out_features=2048)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        self.fc3 = nn.Linear(in_features=2048, out_features=num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv1(input)\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = self.conv2(output)\n",
    "        output = self.bn2(output)\n",
    "        output = self.relu2(output)\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = self.conv3(output)\n",
    "        output = self.bn3(output)\n",
    "        output = self.relu3(output)\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = output.view(-1, 256 * 18 * 18)\n",
    "        output = self.fc1(output)\n",
    "        output = self.relu4(output)\n",
    "        output = self.fc2(output)\n",
    "        output = self.relu5(output)\n",
    "        output = self.fc3(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "cf3e954b-dfc0-4e0a-9c1e-7f1736ca9e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ConvNet10M(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e1f7296c-42d1-4592-b2d9-1b503ca3908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optmizer and loss function\n",
    "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
    "loss_function=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "225b997a-76a3-4260-b778-5602191d3282",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4590182d-8148-4314-9926-a4f6adeb7a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the size of training and testing images\n",
    "train_count=len(glob.glob(train_path+'/**/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "333f3e99-1053-46a8-aa9e-53d1925c0c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12218\n"
     ]
    }
   ],
   "source": [
    "print(train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c0d04a05-d192-457b-85a1-129c06e932ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: tensor(0.0886) Train Accuracy: 0.9719266655753805 Test Accuracy: 0.9471777269260107\n",
      "Epoch: 1 Train Loss: tensor(0.0817) Train Accuracy: 0.9747912915370764 Test Accuracy: 0.9172387490465294\n",
      "Epoch: 2 Train Loss: tensor(0.1097) Train Accuracy: 0.967670649860861 Test Accuracy: 0.9010297482837528\n",
      "Epoch: 3 Train Loss: tensor(0.1112) Train Accuracy: 0.9638238664265837 Test Accuracy: 0.9584286803966438\n",
      "Epoch: 4 Train Loss: tensor(0.0777) Train Accuracy: 0.9749549844491734 Test Accuracy: 0.9548054919908466\n",
      "Epoch: 5 Train Loss: tensor(0.0873) Train Accuracy: 0.9728269765919135 Test Accuracy: 0.9719679633867276\n",
      "Epoch: 6 Train Loss: tensor(0.0920) Train Accuracy: 0.9724995907677197 Test Accuracy: 0.9311594202898551\n",
      "Epoch: 7 Train Loss: tensor(0.0899) Train Accuracy: 0.9716811262072352 Test Accuracy: 0.9553775743707094\n",
      "Epoch: 8 Train Loss: tensor(0.0793) Train Accuracy: 0.9761826812899002 Test Accuracy: 0.9742562929061785\n",
      "Epoch: 9 Train Loss: tensor(0.1013) Train Accuracy: 0.9660337207398919 Test Accuracy: 0.9666285278413425\n"
     ]
    }
   ],
   "source": [
    "#Model training and saving best model\n",
    "\n",
    "best_accuracy=0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #Evaluation and training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy=0.0\n",
    "    train_loss=0.0\n",
    "    \n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs=model(images)\n",
    "        loss=loss_function(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        train_loss+= loss.cpu().data*images.size(0)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        \n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    train_accuracy=train_accuracy/train_count\n",
    "    train_loss=train_loss/train_count\n",
    "    \n",
    "    # print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy))\n",
    "\n",
    "    \n",
    "    # Evaluation on testing dataset\n",
    "    model.eval()\n",
    "    \n",
    "    test_accuracy=0.0\n",
    "    for i, (images,labels) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        outputs=model(images)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "    \n",
    "    test_accuracy=test_accuracy/test_count\n",
    "    \n",
    "    \n",
    "    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\n",
    "\n",
    "    #Save the best model\n",
    "    if test_accuracy>best_accuracy:\n",
    "        torch.save(model.state_dict(),'best_checkpoint.model')\n",
    "        best_accuracy=test_accuracy\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b67540-df05-4f2e-be30-e9d82c9b1789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
