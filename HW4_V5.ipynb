{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f668866b-3d18-4cdb-982c-7620c2310277",
   "metadata": {},
   "source": [
    "# ME 592 Homework 4\n",
    "## Jake Bergfeld, Mohammad Rashid Mohammad Shoaib, Melika Tajipour\n",
    "#### Engineering Image Analysis - Distracted Driving Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23bf84f-aa9b-4c03-a813-0cc567b7eb1d",
   "metadata": {},
   "source": [
    "##### Gathering data from Kaggle - Link to data: https://www.kaggle.com/competitions/state-farm-distracted-driver-detection/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f366cb74-6968-4bdc-a682-809936a7fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle\n",
    "# !mkdir .kaggle  #naming required by kaggle API, creates a hidden folder\n",
    "# !cp /home/exouser/Downloads/kaggle.json /home/exouser/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af339634-b8fb-4500-afb5-9e10930a4faf",
   "metadata": {},
   "source": [
    "##### <u>Confirming location of Kaggle API token was moved successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0b17d81-48f1-4ffc-9742-ec0efc1bb5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd .kaggle && ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93d62f-8c6e-4a4e-b854-fd3f45e0d627",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### <u>Downloading the specific dataset and confirming locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52f27c40-28c7-4ea4-9997-f23b63576322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets list -s 'State Farm Distracted Driver Detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "908d63c8-fd0f-42dd-8d85-5d3f2498ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets download -d 'rightway11/state-farm-distracted-driver-detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4e555fa-03f7-4ac1-96db-d09b39dceb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install unzip\n",
    "# !unzip state-farm-distracted-driver-detection.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfd5be0c-a808-42d2-8527-cb92926eb100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd data && ls\n",
    "# !cd data/imgs && ls\n",
    "# !cd data/imgs/train && ls\n",
    "# !cd data/imgs/test && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6e19d85-a8d7-42e7-9658-e5f1dd2e1ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import glob\n",
    "from torchvision.transforms import transforms\n",
    "# from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2687c4de-b7db-44ca-88bd-78a48a1baa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "261a30d3-4797-4656-98f0-dce695c10c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_10206.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_27079.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_50749.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_97089.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_37741.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34919</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_15827.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34920</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_16688.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34921</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_64532.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34922</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_7918.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34923</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_84918.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34924 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject classname            img\n",
       "0        p012        c0  img_10206.jpg\n",
       "1        p012        c0  img_27079.jpg\n",
       "2        p012        c0  img_50749.jpg\n",
       "3        p012        c0  img_97089.jpg\n",
       "4        p012        c0  img_37741.jpg\n",
       "...       ...       ...            ...\n",
       "34919    p075        c9  img_15827.jpg\n",
       "34920    p075        c9  img_16688.jpg\n",
       "34921    p075        c9  img_64532.jpg\n",
       "34922    p075        c9   img_7918.jpg\n",
       "34923    p075        c9  img_84918.jpg\n",
       "\n",
       "[34924 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load driver details and image filenames\n",
    "driver_imgs = pd.read_csv('/home/exouser/data/driver_imgs_list.csv')\n",
    "driver_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabefe56-95ad-4af1-9ca8-c2d73047ff36",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Data Information: \n",
    "    Default image size is 320x240\n",
    "    \n",
    "    The 10 classes to predict are:\n",
    "        c0: normal driving\n",
    "        c1: texting - right\n",
    "        c2: talking on the phone - right\n",
    "        c3: texting - left\n",
    "        c4: talking on the phone - left\n",
    "        c5: operating the radio\n",
    "        c6: drinking\n",
    "        c7: reaching behind\n",
    "        c8: hair and makeup\n",
    "        c9: talking to passenger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cf7055-8831-48a0-8e97-d438ddd51f2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Step 1: Train a model with roughly 500,000 parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f816f658-c0b1-4857-b150-09636a3176f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### *Formula to calculate the number of parameters in a CNN:*\n",
    "   -  Convolutional layer: (in_channels x out_channels x kernel_height x kernel_width) + out_channels\n",
    "   -  Batch normalization layer: 2 x num_features\n",
    "   -  ReLU activation layer: 0 (no parameters)\n",
    "   -  Max pooling layer: 0 (no parameters)\n",
    "   -  Fully connected layer: (in_features x out_features) + out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8848819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98d72611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#checking for device\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3068b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms\n",
    "transformer=transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
    "                        [0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23f61d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataloader\n",
    "\n",
    "#Path for training and testing directory\n",
    "train_path='/home/exouser/data/imgs/train'\n",
    "# test_path='/home/exouser/data/imgs/test'\n",
    "\n",
    "\n",
    "train_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path,transform=transformer),\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "# test_loader=DataLoader(\n",
    "#     torchvision.datasets.ImageFolder(test_path,transform=transformer),\n",
    "#     batch_size=32, shuffle=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1451f45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n"
     ]
    }
   ],
   "source": [
    "#categories\n",
    "root=pathlib.Path(train_path)\n",
    "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "print(classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ec22587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Network\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,num_classes=10):\n",
    "        super(ConvNet,self).__init__()\n",
    "        \n",
    "        #Output size after convolution filter\n",
    "        #((w-f+2P)/s) +1\n",
    "        \n",
    "        #Input shape= (256,3,150,150)\n",
    "        \n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (256,12,150,150)\n",
    "        self.bn1=nn.BatchNorm2d(num_features=12)\n",
    "        #Shape= (256,12,150,150)\n",
    "        self.relu1=nn.ReLU()\n",
    "        #Shape= (256,12,150,150)\n",
    "        \n",
    "        self.pool=nn.MaxPool2d(kernel_size=2)\n",
    "        #Reduce the image size be factor 2\n",
    "        #Shape= (256,12,75,75)\n",
    "        \n",
    "        \n",
    "        self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (256,20,75,75)\n",
    "        self.relu2=nn.ReLU()\n",
    "        #Shape= (256,20,75,75)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (256,32,75,75)\n",
    "        self.bn3=nn.BatchNorm2d(num_features=32)\n",
    "        #Shape= (256,32,75,75)\n",
    "        self.relu3=nn.ReLU()\n",
    "        #Shape= (256,32,75,75)\n",
    "        \n",
    "        \n",
    "        self.fc=nn.Linear(in_features=75 * 75 * 32,out_features=num_classes)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Feed forwad function\n",
    "        \n",
    "    def forward(self,input):\n",
    "        output=self.conv1(input)\n",
    "        output=self.bn1(output)\n",
    "        output=self.relu1(output)\n",
    "            \n",
    "        output=self.pool(output)\n",
    "            \n",
    "        output=self.conv2(output)\n",
    "        output=self.relu2(output)\n",
    "            \n",
    "        output=self.conv3(output)\n",
    "        output=self.bn3(output)\n",
    "        output=self.relu3(output)\n",
    "            \n",
    "            \n",
    "            #Above output will be in matrix form, with shape (256,32,75,75)\n",
    "            \n",
    "        output=output.view(-1,32*75*75)\n",
    "            \n",
    "            \n",
    "        output=self.fc(output)\n",
    "            \n",
    "        return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e782adb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ConvNet(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3545e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optmizer and loss function\n",
    "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
    "loss_function=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca96e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3043254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the size of training and testing images\n",
    "train_count=len(glob.glob(train_path+'/**/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a96c496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17462\n"
     ]
    }
   ],
   "source": [
    "print(train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d852086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: tensor(4.1442) Train Accuracy: 0.7526629252090253\n",
      "Epoch: 1 Train Loss: tensor(0.3981) Train Accuracy: 0.9391822242583896\n",
      "Epoch: 2 Train Loss: tensor(0.2573) Train Accuracy: 0.9630626503264231\n",
      "Epoch: 3 Train Loss: tensor(0.2222) Train Accuracy: 0.9690184400412324\n",
      "Epoch: 4 Train Loss: tensor(0.1986) Train Accuracy: 0.972683541404192\n",
      "Epoch: 5 Train Loss: tensor(0.1697) Train Accuracy: 0.9794983392509449\n",
      "Epoch: 6 Train Loss: tensor(0.0989) Train Accuracy: 0.9860840682625129\n",
      "Epoch: 7 Train Loss: tensor(0.0717) Train Accuracy: 0.989405566372695\n",
      "Epoch: 8 Train Loss: tensor(0.1549) Train Accuracy: 0.9831061734051082\n",
      "Epoch: 9 Train Loss: tensor(0.0585) Train Accuracy: 0.9905509105486199\n"
     ]
    }
   ],
   "source": [
    "#Model training and saving best model\n",
    "\n",
    "best_accuracy=0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #Evaluation and training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy=0.0\n",
    "    train_loss=0.0\n",
    "    \n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs=model(images)\n",
    "        loss=loss_function(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        train_loss+= loss.cpu().data*images.size(0)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        \n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    train_accuracy=train_accuracy/train_count\n",
    "    train_loss=train_loss/train_count\n",
    "    \n",
    "    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy))\n",
    "\n",
    "    \n",
    "#     # Evaluation on testing dataset\n",
    "#     model.eval()\n",
    "    \n",
    "#     test_accuracy=0.0\n",
    "#     for i, (images,labels) in enumerate(test_loader):\n",
    "#         if torch.cuda.is_available():\n",
    "#             images=Variable(images.cuda())\n",
    "#             labels=Variable(labels.cuda())\n",
    "            \n",
    "#         outputs=model(images)\n",
    "#         _,prediction=torch.max(outputs.data,1)\n",
    "#         test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "    \n",
    "#     test_accuracy=test_accuracy/test_count\n",
    "    \n",
    "    \n",
    "#     print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\n",
    "#     print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy))\n",
    "\n",
    "#     #Save the best model\n",
    "#     if test_accuracy>best_accuracy:\n",
    "#         torch.save(model.state_dict(),'best_checkpoint.model')\n",
    "#         best_accuracy=test_accuracy\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55cb290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
