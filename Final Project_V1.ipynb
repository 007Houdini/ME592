{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f668866b-3d18-4cdb-982c-7620c2310277",
   "metadata": {},
   "source": [
    "# ME 592 Final Project\n",
    "## Jake Bergfeld, Mohammad Rashid Mohammad Shoaib, Melika Tajipour\n",
    "#### Engineering Image Analysis - Automated Chest X-ray classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23bf84f-aa9b-4c03-a813-0cc567b7eb1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Gathering data from Kaggle - Link to data: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f366cb74-6968-4bdc-a682-809936a7fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle\n",
    "# !mkdir .kaggle  #naming required by kaggle API, creates a hidden folder\n",
    "# !cp /home/exouser/Downloads/kaggle.json /home/exouser/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af339634-b8fb-4500-afb5-9e10930a4faf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### <u>Confirming location of Kaggle API token was moved successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b17d81-48f1-4ffc-9742-ec0efc1bb5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd .kaggle && ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93d62f-8c6e-4a4e-b854-fd3f45e0d627",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### <u>Downloading the specific dataset and confirming locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52f27c40-28c7-4ea4-9997-f23b63576322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/exouser/.kaggle/kaggle.json'\n",
      "ref                                                  title                                           size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
      "---------------------------------------------------  ---------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
      "paultimothymooney/chest-xray-pneumonia               Chest X-Ray Images (Pneumonia)                   2GB  2018-03-24 19:41:59         220992       5651  0.75             \n",
      "tolgadincer/labeled-chest-xray-images                Chest X-ray Images                               1GB  2020-08-17 17:32:18           6362        114  0.8125           \n",
      "tawsifurrahman/covid19-radiography-database          COVID-19 Radiography Database                  778MB  2022-03-19 13:38:42          64249        823  1.0              \n",
      "nih-chest-xrays/data                                 NIH Chest X-rays                                42GB  2018-02-21 20:52:23          70090       1015  0.7352941        \n",
      "pcbreviglieri/pneumonia-xray-images                  Pneumonia X-Ray Images                           1GB  2020-05-18 14:32:09           6424         76  0.9375           \n",
      "andyczhao/covidx-cxr2                                COVIDx CXR-2                                    13GB  2022-06-02 13:22:23           7185        165  0.875            \n",
      "praveengovi/coronahack-chest-xraydataset             CoronaHack -Chest X-Ray-Dataset                  1GB  2020-03-20 01:26:40          17699        515  0.9411765        \n",
      "nih-chest-xrays/sample                               Random Sample of NIH Chest X-ray Dataset         4GB  2017-11-23 02:58:24          15740        244  0.7647059        \n",
      "alifrahman/covid19-chest-xray-image-dataset          COVID-19 Chest X-ray Image Dataset              41MB  2020-09-20 19:49:35           3479         65  0.875            \n",
      "vbookshelf/pneumothorax-chest-xray-images-and-masks  Chest X-Ray Images with Pneumothorax Masks       4GB  2020-01-03 11:34:54           1667         27  0.64705884       \n",
      "tawsifurrahman/tuberculosis-tb-chest-xray-dataset    Tuberculosis (TB) Chest X-ray Database         663MB  2021-06-14 09:58:38           8607        141  0.9375           \n",
      "jtiptj/chest-xray-pneumoniacovid19tuberculosis       Chest X-Ray (Pneumonia,Covid-19,Tuberculosis)    2GB  2021-09-16 06:21:54           4420         63  0.8125           \n",
      "preetviradiya/covid19-radiography-dataset            COVID-19 Radiography Dataset                   747MB  2021-05-22 05:04:10           3889         64  1.0              \n",
      "nikhilpandey360/chest-xray-masks-and-labels          Chest Xray Masks and Labels                     10GB  2019-01-21 09:11:43          11033        145  0.75             \n",
      "bachrr/covid-chest-xray                              COVID-19 chest xray                            241MB  2020-05-15 00:30:50          11210        226  0.9411765        \n",
      "prashant268/chest-xray-covid19-pneumonia             Chest X-ray (Covid-19 & Pneumonia)               2GB  2020-09-17 12:08:47          10332        142  0.5              \n",
      "hmchuong/xray-bone-shadow-supression                 X-ray Bone Shadow Supression                     5GB  2018-10-23 15:24:19           1836         45  0.75             \n",
      "raddar/chest-xrays-indiana-university                Chest X-rays (Indiana University)               13GB  2020-02-17 19:25:44           3476         68  0.8235294        \n",
      "andrewmvd/pediatric-pneumonia-chest-xray             Pediatric Pneumonia Chest X-ray                  1GB  2020-03-18 09:11:56           1416         44  0.875            \n",
      "kmader/pulmonary-chest-xray-abnormalities            Pulmonary Chest X-Ray Abnormalities              4GB  2018-03-09 17:46:12           8511        189  0.625            \n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets list -s 'Chest X-ray images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "908d63c8-fd0f-42dd-8d85-5d3f2498ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets download -d 'paultimothymooney/chest-xray-pneumonia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4e555fa-03f7-4ac1-96db-d09b39dceb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install unzip\n",
    "# !unzip chest-xray-pneumonia.zip /home/exouser/ME592/Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8754332c-ab4c-4fe1-b78c-9badb0ef26d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### <u>Data Information: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1e33ac-8e2d-45b2-9e5a-f9e47b7bf167",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Input image size varies\n",
    "    \n",
    "##### The 2 classes to predict are:\n",
    " - Normal/Healthy lungs\n",
    " - Pneumonia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f816f658-c0b1-4857-b150-09636a3176f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### *Formula to calculate the number of parameters in a CNN:*\n",
    "   -  Convolutional layer: (in_channels x out_channels x kernel_height x kernel_width) + out_channels\n",
    "   -  Batch normalization layer: 2 x num_features\n",
    "   -  ReLU activation layer: 0 (no parameters)\n",
    "   -  Max pooling layer: 0 (no parameters)\n",
    "   -  Fully connected layer: (in_features x out_features) + out_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3191612b-b2f7-49bc-9511-95e7af33e161",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Library Imports & Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8848819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98d72611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#checking for device\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3068b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the input images\n",
    "transformer=transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
    "                        [0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6460334-571a-4c5d-a2b6-a424446aa3fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### <u>Generating Test Data\n",
    "#### Only needs to be run one time at the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c35a7cb6-a911-45bf-9be8-014438fe8e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Only run this once to generate test and train data and create separate folders for each\n",
    "\n",
    "# import os\n",
    "# import random\n",
    "# import shutil\n",
    "\n",
    "# # Set the path to the 'Train & Test' folder\n",
    "# folder_path = '/home/exouser/data/imgs/Train & Test'\n",
    "\n",
    "# # Set the path to the 'Train' folder\n",
    "# train_path = '/home/exouser/data/imgs/Train & Test/Train'\n",
    "\n",
    "# # Set the path to the 'Test' folder\n",
    "# test_path = '/home/exouser/data/imgs/Train & Test/Test'\n",
    "\n",
    "# # Loop through each subfolder in the 'Train & Test' folder\n",
    "# for subfolder in os.listdir(folder_path):\n",
    "\n",
    "#     # Create a new subfolder in the 'Train' folder with the same name\n",
    "#     train_subfolder = os.path.join(train_path, subfolder)\n",
    "#     os.makedirs(train_subfolder, exist_ok=True)\n",
    "\n",
    "#     # Create a new subfolder in the 'Test' folder with the same name\n",
    "#     test_subfolder = os.path.join(test_path, subfolder)\n",
    "#     os.makedirs(test_subfolder, exist_ok=True)\n",
    "\n",
    "#     # List all the image files in the subfolder\n",
    "#     images = os.listdir(os.path.join(folder_path, subfolder))\n",
    "#     random.shuffle(images)\n",
    "\n",
    "#     # Calculate the number of images to move to the 'Train' folder\n",
    "#     split_index = int(len(images) * 0.7)\n",
    "\n",
    "#     # Move the first 70% of images to the 'Train' folder\n",
    "#     for image in images[:split_index]:\n",
    "#         source = os.path.join(folder_path, subfolder, image)\n",
    "#         destination = os.path.join(train_subfolder, image)\n",
    "#         shutil.copyfile(source, destination)\n",
    "\n",
    "#     # Move the remaining 30% of images to the 'Test' folder\n",
    "#     for image in images[split_index:]:\n",
    "#         source = os.path.join(folder_path, subfolder, image)\n",
    "#         destination = os.path.join(test_subfolder, image)\n",
    "#         shutil.copyfile(source, destination)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6737a4a2-0e5e-4138-93cc-2708be9b71b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Dataloader and Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23f61d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataloader\n",
    "\n",
    "#Directory path for the training & test images\n",
    "train_path='/home/exouser/ME592/Final Project/chest-xray-pneumonia/train'\n",
    "test_path='/home/exouser/ME592/Final Project/chest-xray-pneumonia/test'\n",
    "\n",
    "train_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path,transform=transformer),\n",
    "    batch_size=16, shuffle=True\n",
    ")\n",
    "test_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(test_path,transform=transformer),\n",
    "    batch_size=16, shuffle=True\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1451f45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NORMAL', 'PNEUMONIA']\n"
     ]
    }
   ],
   "source": [
    "#categories\n",
    "root=pathlib.Path(train_path)\n",
    "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "print(classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a48aa72-d6a0-4fbc-a032-0466c2def2d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Step 1: Build and train a CNN model with roughly 500,000 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f66a126-b1d8-437b-b35c-ba0687eebf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a CNN Network with ~500,000 parameters\n",
    "\n",
    "class ConvNet500k(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet500k, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=128 * 18 * 18, out_features=1024)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv1(input)\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = self.conv2(output)\n",
    "        output = self.bn2(output)\n",
    "        output = self.relu2(output)\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = self.conv3(output)\n",
    "        output = self.bn3(output)\n",
    "        output = self.relu3(output)\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = output.view(-1, 128 * 18 * 18)\n",
    "        output = self.fc1(output)\n",
    "        output = self.relu4(output)\n",
    "        output = self.fc2(output)\n",
    "\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "46da7c29-acae-480a-99b1-9db90d797efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ConvNet500k(num_classes=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "45426278-69e0-4594-8f61-2daebb00c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optmizer and loss function\n",
    "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
    "loss_function=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72f44e2a-937d-4915-9110-0adb7a0e3d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8bb34ac1-2e0e-4459-9d90-b8f6866b61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the size of training and testing images\n",
    "train_count=len(glob.glob(train_path+'/**/*.jpeg'))\n",
    "test_count=len(glob.glob(test_path+'/**/*.jpeg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a70464a-7d9b-4734-b181-9c5cfb826d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5216\n",
      "624\n"
     ]
    }
   ],
   "source": [
    "print(train_count)\n",
    "print(test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "12fa5655-120b-41c0-b1d7-4ccb6769efb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: tensor(0.6605) Train Accuracy: 0.9319401840490797 Test Accuracy: 0.8397435897435898\n",
      "Epoch: 1 Train Loss: tensor(0.0943) Train Accuracy: 0.9651073619631901 Test Accuracy: 0.7740384615384616\n",
      "Epoch: 2 Train Loss: tensor(0.0818) Train Accuracy: 0.9683665644171779 Test Accuracy: 0.8012820512820513\n",
      "Epoch: 3 Train Loss: tensor(0.0826) Train Accuracy: 0.9710506134969326 Test Accuracy: 0.7387820512820513\n",
      "Epoch: 4 Train Loss: tensor(0.0790) Train Accuracy: 0.9716257668711656 Test Accuracy: 0.8285256410256411\n",
      "Epoch: 5 Train Loss: tensor(0.0647) Train Accuracy: 0.9748849693251533 Test Accuracy: 0.7467948717948718\n",
      "Epoch: 6 Train Loss: tensor(0.0543) Train Accuracy: 0.9796779141104295 Test Accuracy: 0.7756410256410257\n",
      "Epoch: 7 Train Loss: tensor(0.0572) Train Accuracy: 0.9789110429447853 Test Accuracy: 0.7596153846153846\n",
      "Epoch: 8 Train Loss: tensor(0.0653) Train Accuracy: 0.9760352760736196 Test Accuracy: 0.7612179487179487\n",
      "Epoch: 9 Train Loss: tensor(0.0603) Train Accuracy: 0.9787193251533742 Test Accuracy: 0.7467948717948718\n"
     ]
    }
   ],
   "source": [
    "#Model training and saving best model\n",
    "\n",
    "best_accuracy=0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #Evaluation and training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy=0.0\n",
    "    train_loss=0.0\n",
    "    \n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs=model(images)\n",
    "        loss=loss_function(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        train_loss+= loss.cpu().data*images.size(0)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        \n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    train_accuracy=train_accuracy/train_count\n",
    "    train_loss=train_loss/train_count\n",
    "    \n",
    "    # print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy))\n",
    "\n",
    "    \n",
    "    # Evaluation on testing dataset\n",
    "    model.eval()\n",
    "    \n",
    "    test_accuracy=0.0\n",
    "    for i, (images,labels) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        outputs=model(images)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "    \n",
    "    test_accuracy=test_accuracy/test_count\n",
    "    \n",
    "    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\n",
    "\n",
    "    #Save the best model\n",
    "    if test_accuracy>best_accuracy:\n",
    "        torch.save(model.state_dict(),'best_checkpoint.model')\n",
    "        best_accuracy=test_accuracy\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050f36d8-66ea-4310-8187-443df0f6d664",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Now using unlabeled images to evaluate the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bcbd763a-5005-4aef-80a6-d24cb625643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading more libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import os\n",
    "import pathlib\n",
    "import glob\n",
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.models import squeezenet1_1\n",
    "from io import open\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab6f6550-301e-443d-bdc9-f427f039e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path='/home/exouser/ME592/Final Project/chest-xray-pneumonia/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe4580c9-63da-4b64-b16b-9a70e185e33b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ConvNet500k:\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([2, 1024]) from checkpoint, the shape in current model is torch.Size([10, 1024]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([10]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16683/3372761246.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_checkpoint.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConvNet500k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2042\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2043\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ConvNet500k:\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([2, 1024]) from checkpoint, the shape in current model is torch.Size([10, 1024]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([10])."
     ]
    }
   ],
   "source": [
    "checkpoint=torch.load('best_checkpoint.model')\n",
    "model=ConvNet500k(num_classes=10)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b5041-9976-4111-9de3-7669fb01620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the prediction input images\n",
    "transformer=transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
    "                        [0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed22f6b3-446d-429e-a405-a6df5f2c2e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction function\n",
    "def prediction(img_path,transformer):\n",
    "    \n",
    "    image=Image.open(img_path)\n",
    "    image_tensor=transformer(image).float()\n",
    "    image_tensor=image_tensor.unsqueeze_(0)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        image_tensor=image_tensor.cuda()\n",
    "        \n",
    "    input=Variable(image_tensor)\n",
    "    \n",
    "    output=model(input)\n",
    "    index=output.data.numpy().argmax()\n",
    "    pred=classes[index]\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e773a-8b23-48ee-90bd-ebf93457d375",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path=glob.glob(pred_path+'/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c662abda-83ec-4636-a0e6-e5e33696f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict={}\n",
    "\n",
    "for i in images_path:\n",
    "    pred_dict[i[i.rfind('/')+1:]]=prediction(i,transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb75946-23eb-4786-91c3-ad654f1d6255",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fbe686-610f-452d-8e80-d72c13700200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c928830d-34e4-4fb8-8317-6174f9556797",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Step 2: Train a model with roughly 10,000,000 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a001f-9752-4a0e-abab-b441f1c21db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet10M(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet10M, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=256)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=256 * 18 * 18, out_features=4096)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=4096, out_features=2048)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        self.fc3 = nn.Linear(in_features=2048, out_features=num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv1(input)\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = self.conv2(output)\n",
    "        output = self.bn2(output)\n",
    "        output = self.relu2(output)\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = self.conv3(output)\n",
    "        output = self.bn3(output)\n",
    "        output = self.relu3(output)\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = output.view(-1, 256 * 18 * 18)\n",
    "        output = self.fc1(output)\n",
    "        output = self.relu4(output)\n",
    "        output = self.fc2(output)\n",
    "        output = self.relu5(output)\n",
    "        output = self.fc3(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3e954b-dfc0-4e0a-9c1e-7f1736ca9e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ConvNet10M(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f7296c-42d1-4592-b2d9-1b503ca3908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optmizer and loss function\n",
    "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
    "loss_function=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225b997a-76a3-4260-b778-5602191d3282",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590182d-8148-4314-9926-a4f6adeb7a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the size of training and testing images\n",
    "train_count=len(glob.glob(train_path+'/**/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f3e99-1053-46a8-aa9e-53d1925c0c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d04a05-d192-457b-85a1-129c06e932ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training and saving best model\n",
    "\n",
    "best_accuracy=0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #Evaluation and training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy=0.0\n",
    "    train_loss=0.0\n",
    "    \n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs=model(images)\n",
    "        loss=loss_function(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        train_loss+= loss.cpu().data*images.size(0)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        \n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    train_accuracy=train_accuracy/train_count\n",
    "    train_loss=train_loss/train_count\n",
    "    \n",
    "    # print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy))\n",
    "\n",
    "    \n",
    "    # Evaluation on testing dataset\n",
    "    model.eval()\n",
    "    \n",
    "    test_accuracy=0.0\n",
    "    for i, (images,labels) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        outputs=model(images)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "    \n",
    "    test_accuracy=test_accuracy/test_count\n",
    "    \n",
    "    \n",
    "    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\n",
    "\n",
    "    #Save the best model\n",
    "    if test_accuracy>best_accuracy:\n",
    "        torch.save(model.state_dict(),'best_checkpoint.model')\n",
    "        best_accuracy=test_accuracy\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b67540-df05-4f2e-be30-e9d82c9b1789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
