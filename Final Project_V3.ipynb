{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f668866b-3d18-4cdb-982c-7620c2310277",
   "metadata": {},
   "source": [
    "# ME 592 Final Project\n",
    "## Jake Bergfeld, Mohammad Rashid Mohammad Shoaib\n",
    "#### Engineering Image Analysis - Automated Chest X-ray classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23bf84f-aa9b-4c03-a813-0cc567b7eb1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Gathering data from Kaggle - Link to data: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f366cb74-6968-4bdc-a682-809936a7fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle\n",
    "# !mkdir .kaggle  #naming required by kaggle API, creates a hidden folder\n",
    "# !cp /home/exouser/Downloads/kaggle.json /home/exouser/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af339634-b8fb-4500-afb5-9e10930a4faf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### <u>Confirming location of Kaggle API token was moved successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b17d81-48f1-4ffc-9742-ec0efc1bb5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd .kaggle && ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93d62f-8c6e-4a4e-b854-fd3f45e0d627",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### <u>Downloading the specific dataset and confirming locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52f27c40-28c7-4ea4-9997-f23b63576322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets list -s 'Chest X-ray images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "908d63c8-fd0f-42dd-8d85-5d3f2498ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets download -d 'paultimothymooney/chest-xray-pneumonia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4e555fa-03f7-4ac1-96db-d09b39dceb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install unzip\n",
    "# !unzip chest-xray-pneumonia.zip /home/exouser/ME592/Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8754332c-ab4c-4fe1-b78c-9badb0ef26d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Data Information: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1e33ac-8e2d-45b2-9e5a-f9e47b7bf167",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Input image size varies\n",
    "    \n",
    "##### The 2 classes to predict are:\n",
    " - Normal/Healthy lungs\n",
    " - Pneumonia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f816f658-c0b1-4857-b150-09636a3176f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### *Formula to calculate the number of parameters in a CNN:*\n",
    "   -  Convolutional layer: (in_channels x out_channels x kernel_height x kernel_width) + out_channels\n",
    "   -  Batch normalization layer: 2 x num_features\n",
    "   -  ReLU activation layer: 0 (no parameters)\n",
    "   -  Max pooling layer: 0 (no parameters)\n",
    "   -  Fully connected layer: (in_features x out_features) + out_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3191612b-b2f7-49bc-9511-95e7af33e161",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Library Imports & Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8848819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98d72611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#checking for device\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3068b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the input images\n",
    "transformer=transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
    "                        [0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c35a7cb6-a911-45bf-9be8-014438fe8e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Only run this once to generate test and train data and create separate folders for each\n",
    "\n",
    "# import os\n",
    "# import random\n",
    "# import shutil\n",
    "\n",
    "# # Set the path to the 'Train & Test' folder\n",
    "# folder_path = '/home/exouser/data/imgs/Train & Test'\n",
    "\n",
    "# # Set the path to the 'Train' folder\n",
    "# train_path = '/home/exouser/data/imgs/Train & Test/Train'\n",
    "\n",
    "# # Set the path to the 'Test' folder\n",
    "# test_path = '/home/exouser/data/imgs/Train & Test/Test'\n",
    "\n",
    "# # Loop through each subfolder in the 'Train & Test' folder\n",
    "# for subfolder in os.listdir(folder_path):\n",
    "\n",
    "#     # Create a new subfolder in the 'Train' folder with the same name\n",
    "#     train_subfolder = os.path.join(train_path, subfolder)\n",
    "#     os.makedirs(train_subfolder, exist_ok=True)\n",
    "\n",
    "#     # Create a new subfolder in the 'Test' folder with the same name\n",
    "#     test_subfolder = os.path.join(test_path, subfolder)\n",
    "#     os.makedirs(test_subfolder, exist_ok=True)\n",
    "\n",
    "#     # List all the image files in the subfolder\n",
    "#     images = os.listdir(os.path.join(folder_path, subfolder))\n",
    "#     random.shuffle(images)\n",
    "\n",
    "#     # Calculate the number of images to move to the 'Train' folder\n",
    "#     split_index = int(len(images) * 0.7)\n",
    "\n",
    "#     # Move the first 70% of images to the 'Train' folder\n",
    "#     for image in images[:split_index]:\n",
    "#         source = os.path.join(folder_path, subfolder, image)\n",
    "#         destination = os.path.join(train_subfolder, image)\n",
    "#         shutil.copyfile(source, destination)\n",
    "\n",
    "#     # Move the remaining 30% of images to the 'Test' folder\n",
    "#     for image in images[split_index:]:\n",
    "#         source = os.path.join(folder_path, subfolder, image)\n",
    "#         destination = os.path.join(test_subfolder, image)\n",
    "#         shutil.copyfile(source, destination)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6737a4a2-0e5e-4138-93cc-2708be9b71b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Dataloader and Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "23f61d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataloader\n",
    "\n",
    "#Directory path for the training & test images\n",
    "train_path='/home/exouser/ME592/Final Project/chest-xray-pneumonia/train'\n",
    "test_path='/home/exouser/ME592/Final Project/chest-xray-pneumonia/test'\n",
    "\n",
    "train_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path,transform=transformer),\n",
    "    batch_size=16, shuffle=True\n",
    ")\n",
    "test_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(test_path,transform=transformer),\n",
    "    batch_size=16, shuffle=True\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1451f45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NORMAL', 'PNEUMONIA']\n"
     ]
    }
   ],
   "source": [
    "#categories\n",
    "root=pathlib.Path(train_path)\n",
    "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "print(classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a48aa72-d6a0-4fbc-a032-0466c2def2d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Step 1: Build and train a CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0f66a126-b1d8-437b-b35c-ba0687eebf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=128 * 18 * 18, out_features=1024)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv1(input)\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = self.conv2(output)\n",
    "        output = self.bn2(output)\n",
    "        output = self.relu2(output)\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = self.conv3(output)\n",
    "        output = self.bn3(output)\n",
    "        output = self.relu3(output)\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = output.view(-1, 128 * 18 * 18)\n",
    "        output = self.fc1(output)\n",
    "        output = self.relu4(output)\n",
    "        output = self.fc2(output)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        #Final layer can be a sigmoid activation layer since only using 2 classes. This will help push the network, on/off functionality. Should help with test acc\n",
    "\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "46da7c29-acae-480a-99b1-9db90d797efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ConvNet(num_classes=2).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "45426278-69e0-4594-8f61-2daebb00c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optmizer and loss function\n",
    "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
    "# loss_function=nn.CrossEntropyLoss()\n",
    "loss_function=nn.BCEWithLogitsLoss()\n",
    "#Since binary problem, can use Binary cross entropy with logits function instead to perform the sigmoid entropy automatically (numerical stability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "72f44e2a-937d-4915-9110-0adb7a0e3d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8bb34ac1-2e0e-4459-9d90-b8f6866b61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the size of training and testing images\n",
    "train_count=len(glob.glob(train_path+'/**/*.jpeg'))\n",
    "test_count=len(glob.glob(test_path+'/**/*.jpeg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0a70464a-7d9b-4734-b181-9c5cfb826d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5216\n",
      "624\n"
     ]
    }
   ],
   "source": [
    "print(train_count)\n",
    "print(test_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "12fa5655-120b-41c0-b1d7-4ccb6769efb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([16, 1])) must be the same as input size (torch.Size([16, 2]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_120001/1571568483.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m         return F.binary_cross_entropy_with_logits(input, target,\n\u001b[0m\u001b[1;32m    721\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3163\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([16, 1])) must be the same as input size (torch.Size([16, 2]))"
     ]
    }
   ],
   "source": [
    "#Model training and saving best model\n",
    "\n",
    "best_accuracy=0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #Evaluation and training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy=0.0\n",
    "    train_loss=0.0\n",
    "    \n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        labels = labels.unsqueeze(1) #ADDING TO ASSIST BCELoss & Sigmoid layer\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs=model(images)\n",
    "        loss=loss_function(outputs.float(),labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        train_loss+= loss.cpu().data*images.size(0)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        \n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    train_accuracy=train_accuracy/train_count\n",
    "    train_loss=train_loss/train_count\n",
    "    \n",
    "    # print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy))\n",
    "\n",
    "    \n",
    "    # Evaluation on testing dataset\n",
    "    model.eval()\n",
    "    \n",
    "    test_accuracy=0.0\n",
    "    for i, (images,labels) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.float().unsqueeze(1).cuda())\n",
    "            \n",
    "        outputs=model(images)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "    \n",
    "    test_accuracy=test_accuracy/test_count\n",
    "    \n",
    "    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\n",
    "\n",
    "    #Save the best model\n",
    "    if test_accuracy>best_accuracy:\n",
    "        torch.save(model.state_dict(),'best_checkpoint.model')\n",
    "        best_accuracy=test_accuracy\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050f36d8-66ea-4310-8187-443df0f6d664",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Now using unlabeled images to evaluate the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bcbd763a-5005-4aef-80a6-d24cb625643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading more libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import os\n",
    "import pathlib\n",
    "import glob\n",
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.models import squeezenet1_1\n",
    "from io import open\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ab6f6550-301e-443d-bdc9-f427f039e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path='/home/exouser/ME592/Final Project/chest-xray-pneumonia/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fe4580c9-63da-4b64-b16b-9a70e185e33b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ConvNet:\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([2, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1024]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_120001/3223127933.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_checkpoint.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConvNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2042\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2043\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ConvNet:\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([2, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1024]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([1])."
     ]
    }
   ],
   "source": [
    "checkpoint=torch.load('best_checkpoint.model')\n",
    "model=ConvNet(num_classes=2)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b5041-9976-4111-9de3-7669fb01620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the prediction input images\n",
    "transformer=transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
    "                        [0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed22f6b3-446d-429e-a405-a6df5f2c2e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction function\n",
    "def prediction(img_path,transformer):\n",
    "    \n",
    "    image=Image.open(img_path)\n",
    "    image_tensor=transformer(image).float()\n",
    "    image_tensor=image_tensor.unsqueeze_(0)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        image_tensor=image_tensor.cuda()\n",
    "        \n",
    "    input=Variable(image_tensor)\n",
    "    \n",
    "    output=model(input)\n",
    "    index=output.data.numpy().argmax()\n",
    "    pred=classes[index]\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e773a-8b23-48ee-90bd-ebf93457d375",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path=glob.glob(pred_path+'/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c662abda-83ec-4636-a0e6-e5e33696f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict={}\n",
    "\n",
    "for i in images_path:\n",
    "    pred_dict[i[i.rfind('/')+1:]]=prediction(i,transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb75946-23eb-4786-91c3-ad654f1d6255",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fbe686-610f-452d-8e80-d72c13700200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "797a001f-9752-4a0e-abab-b441f1c21db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet10M(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ConvNet10M, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=256)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=256 * 18 * 18, out_features=4096)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=4096, out_features=2048)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        self.fc3 = nn.Linear(in_features=2048, out_features=num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv1(input)\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = self.conv2(output)\n",
    "        output = self.bn2(output)\n",
    "        output = self.relu2(output)\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = self.conv3(output)\n",
    "        output = self.bn3(output)\n",
    "        output = self.relu3(output)\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = output.view(-1, 256 * 18 * 18)\n",
    "        output = self.fc1(output)\n",
    "        output = self.relu4(output)\n",
    "        output = self.fc2(output)\n",
    "        output = self.relu5(output)\n",
    "        output = self.fc3(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf3e954b-dfc0-4e0a-9c1e-7f1736ca9e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ConvNet10M(num_classes=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f7296c-42d1-4592-b2d9-1b503ca3908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optmizer and loss function\n",
    "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
    "loss_function=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225b997a-76a3-4260-b778-5602191d3282",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590182d-8148-4314-9926-a4f6adeb7a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the size of training and testing images\n",
    "train_count=len(glob.glob(train_path+'/**/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f3e99-1053-46a8-aa9e-53d1925c0c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d04a05-d192-457b-85a1-129c06e932ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training and saving best model\n",
    "\n",
    "best_accuracy=0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #Evaluation and training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy=0.0\n",
    "    train_loss=0.0\n",
    "    \n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs=model(images)\n",
    "        loss=loss_function(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        train_loss+= loss.cpu().data*images.size(0)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        \n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    train_accuracy=train_accuracy/train_count\n",
    "    train_loss=train_loss/train_count\n",
    "    \n",
    "    # print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy))\n",
    "\n",
    "    \n",
    "    # Evaluation on testing dataset\n",
    "    model.eval()\n",
    "    \n",
    "    test_accuracy=0.0\n",
    "    for i, (images,labels) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        outputs=model(images)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "    \n",
    "    test_accuracy=test_accuracy/test_count\n",
    "    \n",
    "    \n",
    "    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\n",
    "\n",
    "    #Save the best model\n",
    "    if test_accuracy>best_accuracy:\n",
    "        torch.save(model.state_dict(),'best_checkpoint.model')\n",
    "        best_accuracy=test_accuracy\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b67540-df05-4f2e-be30-e9d82c9b1789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
