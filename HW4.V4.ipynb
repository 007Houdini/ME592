{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc745fb-d1fa-475c-944e-215298dfe613",
   "metadata": {},
   "source": [
    "## ME 592 Homework 4 Engineering Image Analysis - Distracted Driving Classification\n",
    "#### Jake Bergfeld, Mohammad Rashid Mohammad Shoaib, Melika Tajipour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09867f5f-1b00-4477-8245-4d922ed60087",
   "metadata": {},
   "source": [
    "#### <u>Gathering data from Kaggle - Link to data: https://www.kaggle.com/competitions/state-farm-distracted-driver-detection/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e4b7a63-0ce7-47b4-a038-688ff98c3622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle\n",
    "# !mkdir .kaggle  #naming required by kaggle API, creates a hidden folder\n",
    "# !cp /home/exouser/Downloads/kaggle.json /home/exouser/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecadb937-2036-451a-b9c2-fccc0d062b0f",
   "metadata": {},
   "source": [
    "#### <u>Confirming location of Kaggle API token was moved successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6cf2e58-4300-439c-b5b4-bbda3ec0efbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle.json\n"
     ]
    }
   ],
   "source": [
    "!cd .kaggle && ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473ec5ea-ba7c-499c-a987-dc15d69df9c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### <u>Downloading the specific dataset and confirming locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4370cac5-3146-4414-9714-1c391e55ba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets list -s 'State Farm Distracted Driver Detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a920556-ff25-40aa-9417-90b3479bc899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets download -d 'rightway11/state-farm-distracted-driver-detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90911ecf-23a3-47b7-8920-32b7465e6398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install unzip\n",
    "# !unzip state-farm-distracted-driver-detection.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24bf6346-382c-4398-b565-65a35f967c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd data && ls\n",
    "# !cd data/imgs && ls\n",
    "# !cd data/imgs/train && ls\n",
    "# !cd data/imgs/test && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf1a9ff1-420b-4a76-8145-1779ea9fb51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import glob\n",
    "from torchvision.transforms import transforms\n",
    "# from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3132210-7bf4-4f66-86b0-ef1952dfecea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_10206.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_27079.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_50749.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_97089.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p012</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_37741.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34919</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_15827.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34920</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_16688.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34921</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_64532.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34922</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_7918.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34923</th>\n",
       "      <td>p075</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_84918.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34924 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject classname            img\n",
       "0        p012        c0  img_10206.jpg\n",
       "1        p012        c0  img_27079.jpg\n",
       "2        p012        c0  img_50749.jpg\n",
       "3        p012        c0  img_97089.jpg\n",
       "4        p012        c0  img_37741.jpg\n",
       "...       ...       ...            ...\n",
       "34919    p075        c9  img_15827.jpg\n",
       "34920    p075        c9  img_16688.jpg\n",
       "34921    p075        c9  img_64532.jpg\n",
       "34922    p075        c9   img_7918.jpg\n",
       "34923    p075        c9  img_84918.jpg\n",
       "\n",
       "[34924 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load driver details and image filenames\n",
    "driver_imgs = pd.read_csv('/home/exouser/data/driver_imgs_list.csv')\n",
    "driver_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3a6776-8e35-4b6e-8060-1dde611c222d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Data Information: \n",
    "    Default image size is 320x240\n",
    "    \n",
    "    The 10 classes to predict are:\n",
    "        c0: normal driving\n",
    "        c1: texting - right\n",
    "        c2: talking on the phone - right\n",
    "        c3: texting - left\n",
    "        c4: talking on the phone - left\n",
    "        c5: operating the radio\n",
    "        c6: drinking\n",
    "        c7: reaching behind\n",
    "        c8: hair and makeup\n",
    "        c9: talking to passenger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed31e1a9-d6c9-4b7c-a350-477bd4c54b60",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <u>Step 1: Train a model with roughly 500,000 parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018ae637-2b0a-4ecb-b85e-bf7ed3c8d4aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### *Formula to calculate the number of parameters in a CNN:*\n",
    "   -  Convolutional layer: (in_channels x out_channels x kernel_height x kernel_width) + out_channels\n",
    "   -  Batch normalization layer: 2 x num_features\n",
    "   -  ReLU activation layer: 0 (no parameters)\n",
    "   -  Max pooling layer: 0 (no parameters)\n",
    "   -  Fully connected layer: (in_features x out_features) + out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55d61b31-1bc1-41f4-a7b5-7f2fb3ff4179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Check for nvidia device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "831bdfa1-452b-4283-bb53-e959db621fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directory path for training & testing data\n",
    "train_path = 'Documents/HW4/state-farm-distracted-driver-detection/imgs/train'\n",
    "test_path = 'Documents/HW4/state-farm-distracted-driver-detection/imgs/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bcfa46a0-3510-4ad1-984a-551202a655e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "str() argument 2 must be str, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4653/2877598046.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Preprocess the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDistractedDriverDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: str() argument 2 must be str, not tuple"
     ]
    }
   ],
   "source": [
    "#Preprocess the data\n",
    "class DistractedDriverDataset(train_path):\n",
    "    def __init__(self, img_dir, driver_csv, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.driver_csv = pd.read_csv(driver_csv)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.driver_csv)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.driver_csv.iloc[idx, 1])\n",
    "        img = Image.open(img_name)\n",
    "        label = self.driver_csv.iloc[idx, 2]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "878b77e9-83fc-4847-b6c7-8905721f79a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data augmentations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e710bf47-2d4f-41bd-9223-53cec838b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directory path for training & testing data\n",
    "train_path = 'Documents/HW4/state-farm-distracted-driver-detection/imgs/train'\n",
    "test_path = 'Documents/HW4/state-farm-distracted-driver-detection/imgs/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b569ae67-da93-4eea-b38e-385ee45a49f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the dataloaders\n",
    "train_loader = DataLoader(train_path, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_path, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3023a0c-b840-405b-b0f0-a8899751fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a CNN with ~500,000 parameters\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(256 * 14 * 14, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = x.view(-1, 256 * 14 * 14)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = CNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "878a0e3c-1368-4cd4-abfa-649b009d3740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For multi-class use a cross-entropy loss and Adam optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d25ab12-e739-4a3e-afd4-c7ed9e0d05b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4653/2628744898.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "        correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print('Accuracy of the network on the test images after epoch %d: %.2f %%' % (epoch + 1, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32760711-33d7-4508-9431-63eaecadb34d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
